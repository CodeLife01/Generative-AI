{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4a5da9",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1d07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## Loading all the enviroment varaible\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f8351bf5400>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f8351bf7770>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05601705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Sadeeq! It's great to hear from a fellow AI enthusiast. \\n\\nAs a Chief AI Engineer, I imagine you're involved in some fascinating projects. What kind of work are you currently focused on? \\n\\nI'm always eager to learn more about the cutting edge of AI development. ðŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 23, 'total_tokens': 96, 'completion_time': 0.132727273, 'prompt_time': 0.002189854, 'queue_time': 0.241706515, 'total_time': 0.134917127}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-1dfc01a4-f98b-409b-8a33-5680264b1c33-0', usage_metadata={'input_tokens': 23, 'output_tokens': 73, 'total_tokens': 96})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Sadeeq and I and a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d2c729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Sadeeq, and you are a Chief AI Engineer. \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm eager to learn more! ðŸ˜Š  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 94, 'total_tokens': 139, 'completion_time': 0.081818182, 'prompt_time': 0.005982004, 'queue_time': 0.245486346, 'total_time': 0.087800186}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f2371d8d-4967-4444-b6d7-c5bb7d39bae7-0', usage_metadata={'input_tokens': 94, 'output_tokens': 45, 'total_tokens': 139})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Sadeeq and I and a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hi Sadeeq, it's nice to meet you! That's a very interesting role. \\n\\nWhat kind of projects are you working on as a Chief AI Engineer? I'm always curious to learn more about the applications of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey Whats my name and what do i do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad987b",
   "metadata": {},
   "source": [
    "### Message History\n",
    "we can use a Message history class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15615b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "## Creating Session for each user using the model\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, My name is Sadeeq and I and a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24159d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Sadeeq, it's nice to meet you!\\n\\nThat's a fascinating title. As a Chief AI Engineer, I imagine you're at the forefront of some exciting developments. What kind of projects are you working on?  \\n\\nI'm eager to learn more about your work in the field of AI.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2188a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Sadeeq.  \\n\\nI remember it from our first exchange! ðŸ˜„  How can I help you today, Sadeeq? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 103, 'total_tokens': 135, 'completion_time': 0.058181818, 'prompt_time': 0.005084376, 'queue_time': 0.243241753, 'total_time': 0.063266194}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a1f6eb5b-fb53-4db0-9769-c10edb7c7041-0', usage_metadata={'input_tokens': 103, 'output_tokens': 32, 'total_tokens': 135})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02db588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to know!\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Change the config (session iD) to see whether the model will remember\n",
    "\n",
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64adefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Muhammad Sadeeq, it's nice to meet you! \\n\\nIs there anything I can help you with today? ðŸ˜Š  \\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey, My name is Muhammad Sadeeq\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8466ce0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Muhammad Sadeeq.  \\n\\nI remembered!  ðŸ˜Š  How can I help you further?\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22884287",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. let's now make that bit more complicated.\n",
    "First, let's add in a system message with some custom instruction (but still taking messages as input). Next we'll add in more input besides jusr the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "368384bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain  = prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a6f2783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Sadeeq! It's nice to meet you.  \\n\\nWhat can I do for you today? ðŸ˜Š  I'm ready to answer your questions and help in any way I can. \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 30, 'total_tokens': 75, 'completion_time': 0.081818182, 'prompt_time': 0.002322175, 'queue_time': 0.296659834, 'total_time': 0.084140357}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3ef5b5a4-5c29-466f-9f34-add9f1fa404c-0', usage_metadata={'input_tokens': 30, 'output_tokens': 45, 'total_tokens': 75})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\": [HumanMessage(content=\"Hi My name is Sadeeq\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0e2cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6a27263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Sadeeq! It's nice to meet you. \\n\\nWhat can I do for you today? ðŸ˜Š  \\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Sadeeq\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30ed0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding More Complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are a helpful assistant. Answer all question to the best of your ability in {language}\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain  = prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28bca260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour Sadeeq ! ðŸ‘‹ \\n\\nEnchantÃ© de te rencontrer. Comment puis-je t'aider aujourd'hui ? ðŸ˜Š \\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"Hi My name is Sadeeq\")], \"language\": \"french\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b602a",
   "metadata": {},
   "source": [
    "lets now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "884fbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ac0d6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sannu da zuwa Sadeeq!  ðŸ˜Š  Ina cikin aiki don taimakawa.\\n\\nShikenan, ka tambayi abin da zaka yi. Wasu tambayoyin da nake iya amsawa sune:\\n\\n* Yana yawa hausa\\n* Tambayoyi game da al'umma\\n* Gudanar da batutuka\\n* Sarrafa bayanai\\n* Kuma yawa da yawa.\\n\\nKamar yadda kike bukata, Sadeeq!  \\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I am Sadeeq\")], \"language\":\"Hausa\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c710c7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anan take sunan ka, Sadeeq! ðŸ˜Š \\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hey, whats my name\")], \"language\":\"Hausa\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc0edb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A tsakiyar yankin Kano, akwai matashi mai suna Fatima, mai kula da shanu. Tana da sha\\'awar yaran da ta kula da su kamar yaranta.\\n\\nA rana mai zafi, Fatima ta samu yarinya mai suna Hadiza, tana kuka a kofar gidanta. Hadiza ta ce ta rasa hanyar gida bayan ta bi kansa mai sauri. Fatima ta ji tausayin Hadiza, ta karbi ta, ta saita ta da ruwan zamani, ta bata abinci, ta kuma ji tsohon almara.\\n\\nSai dai, lokacin da Fatima ta fara bin hanyar da Hadiza ta ce ta zo daga, ta gano cewa ba ta san inda take ba. Fatima ta rasa abinda zata yi, sai dai ta yi tambayar mutanen da suka fuskanta. \\n\\nA karshe, mutane sun taimaka mata, suka nuna mata hanyar da Hadiza ta zo daga. Fatima ta yi farin ciki sosai, ta juya Hadiza ta gida, ta kuma yi alkawarin cewa za ta kula da ita kamar yarinta.\\n\\nHadiza ta yi murmushi a fuskarta, ta goya Fatima da goge, ta ce \"Na gode sosai, Fatima.\" Fatima ta ce \"A\\'a. Ina jin farin ciki ne saboda na iya taimakawa.\"\\n\\nA haka ne Fatima ta yi martabar iyawon mata, ta kuma nuna cewa kin yi gogewa da tausayi a rayuwa.\\n\\n\\nBa haka ba?\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hey, Tell me a short story\")], \"language\":\"Hausa\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0244f",
   "metadata": {},
   "source": [
    "### Manage the conservation History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af990e",
   "metadata": {},
   "source": [
    "One important concept of understanding when building chatbot is how to manage conversation history. If left unmanaged, the messages will grow unbounded and potentially overfloe the content window of the LLM. Therefore, it is important to add a step that limits the size of the messages yor are passin in.\n",
    "\n",
    "\"trim_ messages\" help to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15e4b658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='i like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes1', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 45, ## Change to 45 from 70\n",
    "    strategy=\"last\",\n",
    "    token_counter = model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on = \"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"i like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"Whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"No problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes1\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18004449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't have access to your personal information, including your taste preferences. \\n\\nWhat's your favorite flavor of ice cream?\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what ice cream do i like\")],\n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "\n",
    "### output = I don't have access to your personal information\n",
    "### Bcos the max_token is set to 45 and and ice is not in the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8718606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"What\\'s 2 + 2?\".  ðŸ˜Š \\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45b57f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the message histoy\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config = {\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a37a347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have memory of past conversations. So I don't know your name.  What would you like me to call you? ðŸ˜Š  \\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hey, whats my name\")], \n",
    "     \"language\":\"English\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4669070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please provide me with the math problem you asked so I can help you! I have no memory of past conversations, so I need you to tell me the problem again. ðŸ˜Š  \\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\":\"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733dfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
