{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c50b06",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c299cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9d0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc29964",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc02769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6df666ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, task='text-generation')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, \n",
    "    max_length=150, \n",
    "    temperature=0.7, \n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    task=\"text-generation\",\n",
    "    )\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09268dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nMachine learning is a type of artificial intelligence that allows computer systems to learn from data and improve their performance on a specific task without being explicitly programmed. It involves the use of algorithms and statistical models to analyze and interpret large amounts of data, and to make predictions or decisions based on that data.\\n\\nMachine learning can be used for a wide variety of tasks, such as image and speech recognition, natural language processing, and predictive modeling. It is used in many industries, including finance, healthcare, and technology, to automate processes, make decisions, and provide insights.\\n\\nThere are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the machine learning algorithm is trained on a labeled dataset, where the correct answer is provided for each example. In unsupervised learning, the algorithm is given an unlabeled dataset and must find patterns and structure on its own. In reinforcement learning, the algorithm learns by interacting with an environment and receiving rewards or penalties for its actions.\\n\\nMachine learning is a rapidly growing field, with many exciting applications and opportunities for innovation. It is an important tool for businesses and organizations looking to improve their operations, make data-driven decisions, and stay competitive in today's fast-paced, data-driven world.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc31988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nGenerative AI refers to a class of artificial intelligence models that can create new content, such as images, text, music, or even 3D models, based on the data they have been trained on. These models work by learning the patterns and structures in the data, and then generating new content that is similar to the original data but is unique and not a direct copy.\\n\\nGenerative AI models are typically trained using techniques such as deep learning, which involves training neural networks on large datasets. The neural network learns to recognize patterns and structures in the data, and can then use this knowledge to generate new content that is similar to the original data.\\n\\nSome examples of generative AI models include deep learning models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Transformers. These models have been used to create a wide variety of content, such as generating realistic images of faces, generating new text based on a given prompt, or generating new music based on a given melody.\\n\\nGenerative AI has many potential applications, such as in art, design, and entertainment, as well as in more practical areas such as creating personalized content for marketing or generating synthetic data for training other AI models. However, it also raises important ethical and societal questions, such as issues around copyright and authorship, and the potential for misuse of these powerful tools.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da5e61f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, task='text-generation')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"google/gemma-2-9b\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, \n",
    "    max_length=150, \n",
    "    temperature=0.7, \n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    task=\"text-generation\",\n",
    "    )\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0924991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-680f4dbf-576a0f0e39c4a78b692d2c4c;65c40e9f-5322-41cb-8ea2-633c9b2b8b2b)\n\n403 Forbidden: None.\nCannot access content at: https://router.huggingface.co/hf-inference/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://router.huggingface.co/hf-inference/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is machine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/langchain_core/language_models/llms.py:387\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     **kwargs: Any,\n\u001b[32m    384\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    385\u001b[39m     config = ensure_config(config)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    398\u001b[39m         .text\n\u001b[32m    399\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/langchain_core/language_models/llms.py:764\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    761\u001b[39m     **kwargs: Any,\n\u001b[32m    762\u001b[39m ) -> LLMResult:\n\u001b[32m    763\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/langchain_core/language_models/llms.py:971\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     run_managers = [\n\u001b[32m    958\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    959\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    979\u001b[39m     run_managers = [\n\u001b[32m    980\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    981\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    989\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/langchain_core/language_models/llms.py:790\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    780\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    781\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     **kwargs: Any,\n\u001b[32m    787\u001b[39m ) -> LLMResult:\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    789\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    798\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    799\u001b[39m         )\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    801\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/langchain_core/language_models/llms.py:1544\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1541\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1543\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1544\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1545\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1546\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1547\u001b[39m     )\n\u001b[32m   1548\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:312\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    309\u001b[39m     invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m] = invocation_params[\n\u001b[32m    310\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     response_text = json.loads(response.decode())[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[32m    320\u001b[39m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:132\u001b[39m, in \u001b[36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m     warning_message += \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + message\n\u001b[32m    131\u001b[39m warnings.warn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:305\u001b[39m, in \u001b[36mInferenceClient.post\u001b[39m\u001b[34m(self, json, data, model, task, stream)\u001b[39m\n\u001b[32m    303\u001b[39m url = provider_helper._prepare_url(\u001b[38;5;28mself\u001b[39m.token, mapped_model)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    304\u001b[39m headers = provider_helper._prepare_headers(\u001b[38;5;28mself\u001b[39m.headers, \u001b[38;5;28mself\u001b[39m.token)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:357\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:473\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    468\u001b[39m     message = (\n\u001b[32m    469\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n\u001b[32m    476\u001b[39m     range_header = response.request.headers.get(\u001b[33m\"\u001b[39m\u001b[33mRange\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: (Request ID: Root=1-680f4dbf-576a0f0e39c4a78b692d2c4c;65c40e9f-5322-41cb-8ea2-633c9b2b8b2b)\n\n403 Forbidden: None.\nCannot access content at: https://router.huggingface.co/hf-inference/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb6f7c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, task='text-generation')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, \n",
    "    max_length=150, \n",
    "    temperature=0.7, \n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    task=\"text-generation\",\n",
    "    )\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9d85f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion: {question}\\nAnswer: Lets think step bu step\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer: Lets think step bu step\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f136dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/.pyenv/versions/langchain-3.13.2/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nRonaldo won the Best FIFA Men's Player award in 2018. This award is given annually to the world's best male football player as voted by fans, national team captains, and international football journalists. Ronaldo won the award for the fifth time, having previously won it in 2008, 2013, 2014, and 2016. The award ceremony took place on September 24, 2018, in London.\\n\\nRonaldo's main competition for the award in 2018 was Mohamed Salah and Luka Modrić. Salah had a phenomenal first season with Liverpool, scoring 44 goals in all competitions and helping the team reach the Champions League final. Modrić, on the other hand, was instrumental in Real Madrid's third consecutive Champions League title and Croatia's run to the World Cup final.\\n\\nIn the end, Ronaldo edged out his competitors to win the award. He scored 21 goals in 28 games for Real Madrid in the 2017-18 season, helping the team to win the Champions League for a fourth consecutive year. He also scored four goals in four games for Portugal at the World Cup in Russia.\\n\\nOther winners at the 2018 FIFA Best awards included Marta, who won the Best FIFA Women's Player award, and Pep Guardiola, who won the Best FIFA Men's Coach award for his work with Manchester City.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "llm.invoke('who won best football player 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138360/719859098.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceBgeEmbeddings(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82955dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416551649570465,\n",
       " 0.012183290906250477,\n",
       " 0.027443939819931984,\n",
       " -0.054828692227602005,\n",
       " 0.02423887699842453,\n",
       " 0.0007662862190045416,\n",
       " 0.06783366203308105,\n",
       " 0.016348298639059067,\n",
       " -0.01895073801279068,\n",
       " 0.012542907148599625,\n",
       " 0.021565014496445656,\n",
       " -0.08793039619922638,\n",
       " 0.0006460539880208671,\n",
       " 0.033270783722400665,\n",
       " 0.0054637533612549305,\n",
       " -0.06037640944123268,\n",
       " 0.05042264237999916,\n",
       " 0.004434807226061821,\n",
       " 0.0009598658652976155,\n",
       " 0.0017405833350494504,\n",
       " 0.0032988202292472124,\n",
       " 0.03167250379920006,\n",
       " -0.048807479441165924,\n",
       " -0.04481915012001991,\n",
       " 0.07132111489772797,\n",
       " -0.00751084927469492,\n",
       " -0.0011259291786700487,\n",
       " -0.01580117829144001,\n",
       " -0.029402390122413635,\n",
       " -0.17224565148353577,\n",
       " -0.03189520537853241,\n",
       " -0.0016291735228151083,\n",
       " 0.018104981631040573,\n",
       " 0.015315393917262554,\n",
       " -0.020729562267661095,\n",
       " -0.008872998878359795,\n",
       " -0.001282262266613543,\n",
       " 0.027276858687400818,\n",
       " -0.010114247910678387,\n",
       " 0.012621620669960976,\n",
       " -0.007077870890498161,\n",
       " -0.016693195328116417,\n",
       " 0.04085586220026016,\n",
       " 0.0239383727312088,\n",
       " -0.02008151076734066,\n",
       " 0.02868114411830902,\n",
       " -0.019400758668780327,\n",
       " -0.014618179760873318,\n",
       " 0.017379634082317352,\n",
       " 0.0041640824638307095,\n",
       " 0.06415649503469467,\n",
       " 0.047683048993349075,\n",
       " 0.0018365011783316731,\n",
       " -8.071921183727682e-05,\n",
       " 0.016596777364611626,\n",
       " 0.011124166660010815,\n",
       " 0.0696943923830986,\n",
       " 0.05182050168514252,\n",
       " 0.05568530037999153,\n",
       " 0.05551542714238167,\n",
       " 0.0005039448151364923,\n",
       " 0.041870586574077606,\n",
       " -0.15344084799289703,\n",
       " 0.05180780589580536,\n",
       " 0.006689810194075108,\n",
       " -0.0316707082092762,\n",
       " -0.009105006232857704,\n",
       " -0.051604703068733215,\n",
       " 0.042508576065301895,\n",
       " 0.028200028464198112,\n",
       " -0.010748126544058323,\n",
       " 0.02240578643977642,\n",
       " 0.044395510107278824,\n",
       " 0.004115525167435408,\n",
       " 0.018998468294739723,\n",
       " -0.004357169847935438,\n",
       " 0.04762760177254677,\n",
       " 0.01182462926954031,\n",
       " 0.008164589293301105,\n",
       " 0.008177279494702816,\n",
       " -0.009698745794594288,\n",
       " -0.014260295778512955,\n",
       " 0.011409704573452473,\n",
       " -0.07362113147974014,\n",
       " -0.054395224899053574,\n",
       " -0.05703964829444885,\n",
       " -0.003608556929975748,\n",
       " 0.002666121581569314,\n",
       " 0.02378247305750847,\n",
       " 0.015376229770481586,\n",
       " -0.0702037364244461,\n",
       " -0.031300388276576996,\n",
       " -0.0031142826192080975,\n",
       " -0.015812167897820473,\n",
       " -0.03791399300098419,\n",
       " -0.02592191845178604,\n",
       " 0.018168412148952484,\n",
       " -0.038824599236249924,\n",
       " -0.05674506723880768,\n",
       " 0.5792059898376465,\n",
       " -0.052788328379392624,\n",
       " 0.02071639709174633,\n",
       " 0.06794389337301254,\n",
       " -0.045416541397571564,\n",
       " 0.011642451398074627,\n",
       " -0.021571749821305275,\n",
       " 0.020341720432043076,\n",
       " -0.027448948472738266,\n",
       " -0.04558897390961647,\n",
       " -0.029443571344017982,\n",
       " -0.02366252988576889,\n",
       " -0.03431526571512222,\n",
       " 0.0019388716900721192,\n",
       " -0.07095137983560562,\n",
       " 0.034556321799755096,\n",
       " -0.03055894374847412,\n",
       " 0.03907860070466995,\n",
       " -0.029707323759794235,\n",
       " -0.0008282953640446067,\n",
       " -0.012159359641373158,\n",
       " -0.01827280782163143,\n",
       " 0.02548649162054062,\n",
       " -0.004461662378162146,\n",
       " 0.01633528061211109,\n",
       " 0.019126519560813904,\n",
       " -0.054832085967063904,\n",
       " 0.0276359710842371,\n",
       " -0.004757678601890802,\n",
       " 0.059001706540584564,\n",
       " -0.001694467500783503,\n",
       " 0.008015024475753307,\n",
       " -0.03772684186697006,\n",
       " -0.09893041849136353,\n",
       " -0.022574396803975105,\n",
       " -0.03760464861989021,\n",
       " -0.0021698649507015944,\n",
       " 0.003244602121412754,\n",
       " -0.019202543422579765,\n",
       " -0.008631229400634766,\n",
       " -0.048023074865341187,\n",
       " 0.008696705102920532,\n",
       " -0.0951610878109932,\n",
       " -0.03496047481894493,\n",
       " -0.04360802471637726,\n",
       " -0.00034401085576973855,\n",
       " -0.010173649527132511,\n",
       " -0.03099953383207321,\n",
       " 0.024309692904353142,\n",
       " -0.02040203846991062,\n",
       " 0.03113941103219986,\n",
       " 0.0008811188745312393,\n",
       " 0.013916457071900368,\n",
       " -0.031196242198348045,\n",
       " -0.03715403750538826,\n",
       " 0.00402966421097517,\n",
       " 0.014799799770116806,\n",
       " 0.04318894073367119,\n",
       " 0.03875482827425003,\n",
       " 0.013851992785930634,\n",
       " 0.019797878339886665,\n",
       " 0.010267076082527637,\n",
       " -0.00543410936370492,\n",
       " -0.014299212023615837,\n",
       " 0.027637837454676628,\n",
       " 0.009802635759115219,\n",
       " -0.1355028599500656,\n",
       " -0.017139775678515434,\n",
       " 0.017617065459489822,\n",
       " 0.023132218047976494,\n",
       " 0.0017590150237083435,\n",
       " 0.030889401212334633,\n",
       " 0.03991870582103729,\n",
       " -0.013684182427823544,\n",
       " 0.024816546589136124,\n",
       " 0.05405019596219063,\n",
       " 0.017761176452040672,\n",
       " -0.018475057557225227,\n",
       " 0.02595536969602108,\n",
       " -0.006377536803483963,\n",
       " -0.01658732257783413,\n",
       " 0.03784802183508873,\n",
       " -0.027290061116218567,\n",
       " -0.05284581333398819,\n",
       " -0.03803319111466408,\n",
       " 0.051911093294620514,\n",
       " -0.00755709083750844,\n",
       " -0.03180531784892082,\n",
       " 0.013284181244671345,\n",
       " -0.02772372215986252,\n",
       " 0.056306563317775726,\n",
       " 0.0030418727546930313,\n",
       " 0.05332484468817711,\n",
       " -0.05791125446557999,\n",
       " -0.01132582500576973,\n",
       " -0.031172025948762894,\n",
       " 0.025608690455555916,\n",
       " 0.03389059379696846,\n",
       " -0.0010284638265147805,\n",
       " 0.015864891931414604,\n",
       " 0.01059520710259676,\n",
       " -0.027037784457206726,\n",
       " -0.0009308481821790338,\n",
       " -0.048152245581150055,\n",
       " 0.02817925624549389,\n",
       " 0.010320611298084259,\n",
       " 0.06662959605455399,\n",
       " -0.016558179631829262,\n",
       " -0.0044313836842775345,\n",
       " 0.03823427855968475,\n",
       " -0.023408185690641403,\n",
       " -0.03558176010847092,\n",
       " -0.05829068273305893,\n",
       " -0.011181475594639778,\n",
       " -0.017684556543827057,\n",
       " -0.016141317784786224,\n",
       " -0.0342453233897686,\n",
       " -0.025139523670077324,\n",
       " 0.039396680891513824,\n",
       " -0.023658234626054764,\n",
       " -0.0077250259928405285,\n",
       " -0.005098923575133085,\n",
       " -0.03523436188697815,\n",
       " -0.014076855033636093,\n",
       " -0.223260298371315,\n",
       " -0.03147134929895401,\n",
       " -0.0012905760668218136,\n",
       " -0.0017200281145051122,\n",
       " -0.007846028544008732,\n",
       " -0.058023229241371155,\n",
       " 0.04617457836866379,\n",
       " 0.024552633985877037,\n",
       " 0.07320840656757355,\n",
       " 0.01726832054555416,\n",
       " 0.04761206731200218,\n",
       " 0.013473288156092167,\n",
       " -0.00551604712381959,\n",
       " -0.01435783226042986,\n",
       " -0.009674306027591228,\n",
       " 0.04878249019384384,\n",
       " 0.03053811378777027,\n",
       " -0.02499394677579403,\n",
       " 0.021486220881342888,\n",
       " 0.017639808356761932,\n",
       " 0.05313890427350998,\n",
       " 0.013485017232596874,\n",
       " -0.02322598733007908,\n",
       " -0.0214039646089077,\n",
       " 0.02607535943388939,\n",
       " 0.0020291768014431,\n",
       " 0.12753744423389435,\n",
       " 0.08316836506128311,\n",
       " 0.04408949986100197,\n",
       " -0.02670356072485447,\n",
       " 0.005522006191313267,\n",
       " -0.009294882416725159,\n",
       " 0.02007431723177433,\n",
       " -0.09684177488088608,\n",
       " -0.02470390312373638,\n",
       " 0.02508697845041752,\n",
       " 0.002088637789711356,\n",
       " -0.04489404335618019,\n",
       " -0.07861137390136719,\n",
       " -0.004376350436359644,\n",
       " -0.06590452045202255,\n",
       " 0.014689420349895954,\n",
       " -0.05764184892177582,\n",
       " -0.07152026891708374,\n",
       " -0.06232648715376854,\n",
       " 0.003431654302403331,\n",
       " -0.04606551304459572,\n",
       " 0.04530090093612671,\n",
       " -0.02676226757466793,\n",
       " 0.03401090204715729,\n",
       " 0.04547379910945892,\n",
       " -0.028179261833429337,\n",
       " 0.005011794622987509,\n",
       " 0.009630824439227581,\n",
       " -0.03030560351908207,\n",
       " -0.0361248143017292,\n",
       " -0.013626988045871258,\n",
       " -0.032653722912073135,\n",
       " -0.04467754065990448,\n",
       " 0.010642195120453835,\n",
       " -0.02748635970056057,\n",
       " -0.024565119296312332,\n",
       " -0.02474776655435562,\n",
       " 0.05361955612897873,\n",
       " 0.020789938047528267,\n",
       " 0.01946849375963211,\n",
       " 0.05324118584394455,\n",
       " -0.014002451673150063,\n",
       " 0.021243266761302948,\n",
       " -0.04957321286201477,\n",
       " -0.0085225785151124,\n",
       " 0.007852859795093536,\n",
       " -0.05719393864274025,\n",
       " -0.027550652623176575,\n",
       " 0.005300882738083601,\n",
       " 0.04007291421294212,\n",
       " 0.019597919657826424,\n",
       " -0.04519733414053917,\n",
       " 0.032435812056064606,\n",
       " -0.012342444621026516,\n",
       " 0.034314434975385666,\n",
       " 0.021102122962474823,\n",
       " 0.039846502244472504,\n",
       " 0.03166380152106285,\n",
       " -0.033590223640203476,\n",
       " 0.03164786845445633,\n",
       " -0.0033045089803636074,\n",
       " 0.004641844425350428,\n",
       " 0.037589360028505325,\n",
       " -0.05924459919333458,\n",
       " 0.0070283422246575356,\n",
       " 0.0038086853455752134,\n",
       " -0.025788893923163414,\n",
       " -0.021203359588980675,\n",
       " 0.022691216319799423,\n",
       " -0.02177296206355095,\n",
       " -0.27963775396347046,\n",
       " 0.007267413195222616,\n",
       " 0.021072009578347206,\n",
       " 0.04519743472337723,\n",
       " -0.020534468814730644,\n",
       " 0.02431371435523033,\n",
       " -0.0006136976880952716,\n",
       " -0.011857017874717712,\n",
       " -0.03296778351068497,\n",
       " 0.03584315627813339,\n",
       " 0.0312817320227623,\n",
       " 0.06373955309391022,\n",
       " 0.046547841280698776,\n",
       " -0.014470524154603481,\n",
       " 0.01586972549557686,\n",
       " 0.03397125378251076,\n",
       " 0.018059581518173218,\n",
       " 0.002298751613125205,\n",
       " 0.016549862921237946,\n",
       " -0.021714895963668823,\n",
       " -0.03485998883843422,\n",
       " -0.0008649306837469339,\n",
       " 0.15126043558120728,\n",
       " -0.02453676424920559,\n",
       " 0.03067120909690857,\n",
       " -0.007318185642361641,\n",
       " -0.006135446019470692,\n",
       " 0.06415148079395294,\n",
       " 0.0160214900970459,\n",
       " -0.03636440634727478,\n",
       " 0.019898606464266777,\n",
       " -0.021172329783439636,\n",
       " 0.048294153064489365,\n",
       " -0.044780999422073364,\n",
       " 0.0476338192820549,\n",
       " 0.0007749302894808352,\n",
       " -0.005927938502281904,\n",
       " 0.06154269725084305,\n",
       " 0.023968406021595,\n",
       " 0.013305027969181538,\n",
       " 0.02268449403345585,\n",
       " 0.014538086019456387,\n",
       " -0.052159059792757034,\n",
       " -0.03274965286254883,\n",
       " 0.08583345264196396,\n",
       " -0.003724820679053664,\n",
       " 0.0013494276208803058,\n",
       " 0.040919892489910126,\n",
       " 0.011659654788672924,\n",
       " 0.05843619629740715,\n",
       " -0.022286195307970047,\n",
       " -0.011520685628056526,\n",
       " 0.004705725237727165,\n",
       " 0.0471826009452343,\n",
       " -0.0019179026130586863,\n",
       " 0.033009372651576996,\n",
       " -0.035050563514232635,\n",
       " -0.020736578851938248,\n",
       " -0.009222174994647503,\n",
       " 0.014618266373872757,\n",
       " 0.006456067319959402,\n",
       " 0.0010978507343679667,\n",
       " 0.010224021971225739,\n",
       " 0.0853722095489502,\n",
       " 0.03883953019976616]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149e64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655ea62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
